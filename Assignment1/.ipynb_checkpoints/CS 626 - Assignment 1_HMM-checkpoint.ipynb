{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/tushar/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package brown to /home/tushar/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tushar/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tset = 'universal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged = brown.tagged_words(tagset=tset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = np.unique([tag for (word,tag) in brown_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(tagset=tset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = len(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sents = brown_tagged_sents[0:int(sents*train_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sents = brown_tagged_sents[int(sents*train_ratio):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpm_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    tpm_dict[tag] = {}\n",
    "    start_dict[tag] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    for suc_tag in unique_tags:\n",
    "        tpm_dict[tag][suc_tag] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in training_sents:\n",
    "    for pos in range(len(sent)-1):\n",
    "        tpm_dict[sent[pos][1]][sent[pos+1][1]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in training_sents:\n",
    "    start_dict[(sent[0][1])]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 2931,\n",
       " 'ADJ': 1800,\n",
       " 'ADP': 6261,\n",
       " 'ADV': 4216,\n",
       " 'CONJ': 2231,\n",
       " 'DET': 10665,\n",
       " 'NOUN': 6800,\n",
       " 'NUM': 869,\n",
       " 'PRON': 6385,\n",
       " 'PRT': 1555,\n",
       " 'VERB': 2139,\n",
       " 'X': 20}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_start_prob(S1):\n",
    "    return np.log(start_dict[S1]/(np.sum(list(start_dict.values()))))\n",
    "    #return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpm_prob(S1,S2):\n",
    "    return tpm_dict[S1][S2]/np.sum(list(tpm_dict[S1].values()))\n",
    "    #return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tpm_prob(S1,S2):\n",
    "    return np.log(tpm_dict[S1][S2]/np.sum(list(tpm_dict[S1].values())))\n",
    "    #return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of lexical probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    lex_dict[tag] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in training_sents:\n",
    "    for word in sent:        \n",
    "        try:\n",
    "            lex_dict[word[1]][word[0]]+=1\n",
    "        except:\n",
    "            lex_dict[word[1]][word[0]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lex_prob(S1,S2):\n",
    "    try:\n",
    "        return lex_dict[S1][S2]/np.sum(list(lex_dict[S1].values()))\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lex_prob(S1,S2):\n",
    "    total_words = np.sum(list(lex_dict[S1].values()))\n",
    "    try:\n",
    "        w_s1 = lex_dict[S1][S2]\n",
    "    except:\n",
    "        return -500\n",
    "    return np.log(w_s1/total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = pd.DataFrame(columns=['pred','actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for sent in testing_sents:\n",
    "    s += 1\n",
    "    prob_dict = {}\n",
    "    for tag in unique_tags:\n",
    "        prob_dict[tag] = log_start_prob(tag)\n",
    "\n",
    "    pos_seq = {}\n",
    "    for tag in unique_tags:\n",
    "        pos_seq[tag] = [tag]\n",
    "\n",
    "    pd.DataFrame(pos_seq)\n",
    "\n",
    "    brown_tagged_sents\n",
    "\n",
    "    for word in sent:\n",
    "        pos_seq_init = pos_seq.copy()\n",
    "        prob_dict_init = prob_dict.copy()\n",
    "        for tag in unique_tags:\n",
    "            p_max = -np.inf\n",
    "            best_tag = 'X'\n",
    "            for prev_tag in unique_tags:\n",
    "                lex_prob = log_lex_prob(prev_tag,word[0])\n",
    "                p_temp = prob_dict_init[prev_tag] + lex_prob + log_tpm_prob(prev_tag,tag)\n",
    "                if(p_temp>p_max):\n",
    "                    p_max = p_temp\n",
    "                    best_tag = prev_tag\n",
    "            prob_dict[tag] = p_max \n",
    "            pos_seq[tag] = pos_seq_init[best_tag] + list([tag])\n",
    "        #print(word)\n",
    "        #display(pd.DataFrame(pos_seq))\n",
    "        #display(pd.DataFrame(pd.Series(prob_dict)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for tag in unique_tags:\n",
    "        p = prob_dict[tag]\n",
    "        p = p - log_tpm_prob(pos_seq[tag][-2],tag)\n",
    "        prob_dict[tag] = p\n",
    "        \n",
    "    tag_seq = max(prob_dict,key=prob_dict.get)\n",
    "    \n",
    "    temp = pd.DataFrame([pos_seq[tag_seq][:-1],[b for (a,b) in sent]]).T.rename(columns={0:'pred',1:'actual'})\n",
    "    \n",
    "    temp['word'] = [a for (a,b) in sent]\n",
    "    \n",
    "    accuracy = accuracy.append(temp)\n",
    "\n",
    "    accuracy = accuracy.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    if(s%20==0):\n",
    "        accuracy.to_pickle('accuracy_final_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    for prev_tag in unique_tags:\n",
    "        print(prev_tag,tag)\n",
    "        print(log_tpm_prob(prev_tag,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
