{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "hF-jhnBQB9_1",
    "outputId": "bf241914-bd91-4dc3-8a6a-d3b0686854f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('popular')\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "tset = 'universal'\n",
    "\n",
    "brown_tagged = brown.tagged_words(tagset=tset)\n",
    "\n",
    "unique_tags = np.unique([tag for (word,tag) in brown_tagged])\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(tagset=tset)\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "sents = len(brown_tagged_sents)\n",
    "\n",
    "division = 4\n",
    "\n",
    "train_count = int(sents*train_ratio)\n",
    "\n",
    "test_count = int(sents*(1-train_ratio))\n",
    "\n",
    "testing_sents = brown_tagged_sents[-division*test_count:-(division-1)*test_count-1]\n",
    "\n",
    "training_sents = list(brown_tagged_sents[0:-division*test_count])\n",
    "training_sents.extend(brown_tagged_sents[-(division-1)*test_count-1:])\n",
    "\n",
    "len(training_sents), len(testing_sents)\n",
    "\n",
    "#### Estimation of transition probabilities\n",
    "\n",
    "tpm_dict = {}\n",
    "\n",
    "start_dict = {}\n",
    "\n",
    "for tag in unique_tags:\n",
    "    tpm_dict[tag] = {}\n",
    "    start_dict[tag] = 0\n",
    "\n",
    "for tag in unique_tags:\n",
    "    for suc_tag in unique_tags:\n",
    "        tpm_dict[tag][suc_tag] = 0\n",
    "\n",
    "for sent in training_sents:\n",
    "    for pos in range(len(sent)-1):\n",
    "        tpm_dict[sent[pos][1]][sent[pos+1][1]]+=1\n",
    "\n",
    "for sent in training_sents:\n",
    "    start_dict[(sent[0][1])]+=1\n",
    "\n",
    "def log_start_prob(S1):\n",
    "    return np.log(start_dict[S1]/(np.sum(list(start_dict.values()))))\n",
    "    #return 0\n",
    "\n",
    "def get_tpm_prob(S1,S2):\n",
    "    return tpm_dict[S1][S2]/np.sum(list(tpm_dict[S1].values()))\n",
    "    #return 1\n",
    "\n",
    "def log_tpm_prob(S1,S2):\n",
    "    return np.log(tpm_dict[S1][S2]/np.sum(list(tpm_dict[S1].values())))\n",
    "    #return 0\n",
    "\n",
    "#### Estimation of lexical probabilities\n",
    "\n",
    "lex_dict = {}\n",
    "\n",
    "for tag in unique_tags:\n",
    "    lex_dict[tag] = {}\n",
    "\n",
    "for sent in training_sents:\n",
    "    for word in sent:        \n",
    "        try:\n",
    "            lex_dict[word[1]][word[0]]+=1\n",
    "        except:\n",
    "            lex_dict[word[1]][word[0]]=1\n",
    "\n",
    "def get_lex_prob(S1,S2):\n",
    "    try:\n",
    "        return lex_dict[S1][S2]/np.sum(list(lex_dict[S1].values()))\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def log_lex_prob(S1,S2):\n",
    "    total_words = np.sum(list(lex_dict[S1].values()))\n",
    "    try:\n",
    "        w_s1 = lex_dict[S1][S2]\n",
    "    except:\n",
    "        return -500\n",
    "    return np.log(w_s1/total_words)\n",
    "\n",
    "#### Viterbi algorithm\n",
    "\n",
    "accuracy = pd.DataFrame(columns=['pred','actual'])\n",
    "\n",
    "s = 0\n",
    "for sent in testing_sents:\n",
    "    s += 1\n",
    "    prob_dict = {}\n",
    "    for tag in unique_tags:\n",
    "        prob_dict[tag] = log_start_prob(tag)\n",
    "\n",
    "    pos_seq = {}\n",
    "    for tag in unique_tags:\n",
    "        pos_seq[tag] = [tag]\n",
    "\n",
    "    pd.DataFrame(pos_seq)\n",
    "\n",
    "    brown_tagged_sents\n",
    "\n",
    "    for word in sent:\n",
    "        pos_seq_init = pos_seq.copy()\n",
    "        prob_dict_init = prob_dict.copy()\n",
    "        for tag in unique_tags:\n",
    "            p_max = -np.inf\n",
    "            best_tag = 'X'\n",
    "            for prev_tag in unique_tags:\n",
    "                lex_prob = log_lex_prob(prev_tag,word[0])\n",
    "                p_temp = prob_dict_init[prev_tag] + lex_prob + log_tpm_prob(prev_tag,tag)\n",
    "                if(p_temp>p_max):\n",
    "                    p_max = p_temp\n",
    "                    best_tag = prev_tag\n",
    "            prob_dict[tag] = p_max \n",
    "            pos_seq[tag] = pos_seq_init[best_tag] + list([tag])\n",
    "        #print(word)\n",
    "        #display(pd.DataFrame(pos_seq))\n",
    "        #display(pd.DataFrame(pd.Series(prob_dict)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for tag in unique_tags:\n",
    "        p = prob_dict[tag]\n",
    "        p = p - log_tpm_prob(pos_seq[tag][-2],tag)\n",
    "        prob_dict[tag] = p\n",
    "        \n",
    "    tag_seq = max(prob_dict,key=prob_dict.get)\n",
    "    \n",
    "    temp = pd.DataFrame([pos_seq[tag_seq][:-1],[b for (a,b) in sent]]).T.rename(columns={0:'pred',1:'actual'})\n",
    "    \n",
    "    temp['word'] = [a for (a,b) in sent]\n",
    "    \n",
    "    accuracy = accuracy.append(temp)\n",
    "\n",
    "    accuracy = accuracy.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    if(s%200==0):\n",
    "        accuracy.to_pickle('accuracy_final_new_'+str(division)+'.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4pxK4CoNCFs9",
    "outputId": "f34b7d7a-5ff9-4235-9197-1889a008ab12"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_6578e75b-bb5e-46d1-ad39-40957b005ea7\", \"accuracy_final_new_4.pkl\", 16368341)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('accuracy_final_new_'+str(division)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "BnJ6QYyuEWxd",
    "outputId": "a5a0eb4c-0ac6-4a88-cd5c-0968403c06b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qrXjZlbeyO2F",
    "outputId": "5ed3b6d6-fe01-427a-d6d7-69a12a045b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHDMNbxbyk1v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HMM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
